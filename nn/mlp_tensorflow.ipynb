{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Raoul Malm  \n",
    "\n",
    "**Abstract:** \n",
    "\n",
    "Implementing different multilayer perceptrons (MLPs) in Tensorflow for simple classification tasks. The key goal here is to get some confidence in visualizing graphs with Tensorboard as well as saving and loading pretrained graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raoul/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.4.1\n",
      "Python version 3.6.5 |Anaconda custom (64-bit)| (default, Mar 29 2018, 13:14:23) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "train data shapes:  (2000, 1) (2000, 2)\n",
      "test data shapes:  (2000, 1) (2000, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import libraries and perform data preprocessing.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import sklearn.model_selection\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# Print current python and tensorflow versions.\n",
    "print('Tensorflow version', tf.__version__)\n",
    "print('Python version', sys.version)\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert dense labels to one-hot-encodings.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "def one_hot_to_dense(labels_one_hot):\n",
    "    \"\"\"Convert one-hot encodings into dense labels.\"\"\"\n",
    "    return np.argmax(labels_one_hot,1)\n",
    "\n",
    "def get_next_mini_batch():\n",
    "    \"\"\"Return the next mini batch of training data.\n",
    "    \n",
    "    Returns\n",
    "        x_trn_mb: mini batch of training x values\n",
    "        y_trn_mb: batch of training y values\n",
    "    \"\"\"\n",
    "    global idx_in_epoch, epoch, ids, x_trn, y_trn\n",
    "    \n",
    "    start = idx_in_epoch\n",
    "    idx_in_epoch += mb_size           \n",
    "    epoch += mb_size/len(x_trn)\n",
    "\n",
    "    # At the start of each epoch.\n",
    "    if start == 0:\n",
    "        np.random.shuffle(ids)  \n",
    "\n",
    "    # In case the current index is larger than one epoch.\n",
    "    if idx_in_epoch > len(x_trn):\n",
    "        idx_in_epoch = 0\n",
    "        epoch -= mb_size/len(x_trn) \n",
    "        return get_next_mini_batch() # Recursive use of function.\n",
    "\n",
    "    # Take mini batch from training samples excluding validation samples\n",
    "    x_trn_mb = x_trn[ids[start:idx_in_epoch]]\n",
    "    y_trn_mb = y_trn[ids[start:idx_in_epoch]]\n",
    "    return x_trn_mb, y_trn_mb\n",
    "\n",
    "# number of training and test samples\n",
    "n_train = 1000 # train samples per class\n",
    "n_test = 1000 # test samples per class\n",
    "n_classes = 2  \n",
    "r_seed = 123\n",
    "n_hid_1 = 10\n",
    "\n",
    "# create distinct classes\n",
    "np.random.seed(r_seed)\n",
    "tf.set_random_seed(r_seed)\n",
    "\n",
    "# create training set\n",
    "x_train = np.reshape(np.concatenate([np.random.normal(-1,1,n_train), np.random.normal(1,1,n_train)]),(-1,1))\n",
    "y_train = np.concatenate([np.array([0]*n_train), np.array([1]*n_train)])\n",
    "\n",
    "# create test set\n",
    "x_test = np.reshape(np.concatenate([np.random.normal(-1,1,n_test), np.random.normal(1,1,n_test)]),(-1,1))\n",
    "y_test = np.concatenate([np.array([0]*n_test), np.array([1]*n_test)])\n",
    "\n",
    "# randomize training and test data\n",
    "ids = np.arange(len(x_train))\n",
    "np.random.shuffle(ids)\n",
    "x_train = x_train[ids]\n",
    "y_train = y_train[ids]\n",
    "\n",
    "ids = np.arange(len(x_test))\n",
    "np.random.shuffle(ids)\n",
    "x_test = x_test[ids]\n",
    "y_test = y_test[ids]\n",
    "\n",
    "# one-hot-encoding\n",
    "y_train = dense_to_one_hot(y_train, n_classes)\n",
    "y_test = dense_to_one_hot(y_test, n_classes)\n",
    "\n",
    "print('train data shapes: ', x_train.shape, y_train.shape)\n",
    "print('test data shapes: ', x_test.shape, y_test.shape)\n",
    "\n",
    "# visualize training data\n",
    "bins = np.linspace(-4, 4, 50)\n",
    "plt.hist([[x for i,x in enumerate(x_train[:,0]) if y_train[i,0]==1],\n",
    "         [x for i,x in enumerate(x_train[:,0]) if y_train[i,1]==1]], \n",
    "         bins=bins, alpha=0.5, label=['class 0', 'class 1'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Toy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "1.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "2.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "3.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "4.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "5.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "6.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "7.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "8.000 epoch: train/valid loss 0.348/0.348, train/valid score 0.854/0.854\n",
      "9.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "10.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "11.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "12.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "13.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "14.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "15.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "16.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "17.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "18.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n",
      "19.000 epoch: train/valid loss 0.347/0.347, train/valid score 0.854/0.854\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple Logistic Regression.\n",
    "Implementation of Gradient Descent in TensorFlow using tf.gradients\n",
    "\"\"\"\n",
    "learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph()  \n",
    "            \n",
    "# Input layer\n",
    "x_data_tf = tf.placeholder(tf.float32, [None, 1], 'x_data_tf')\n",
    "y_data_tf = tf.placeholder(tf.int32, [None, n_classes], 'y_data_tf')\n",
    "\n",
    "# Output layer\n",
    "w1_tf = tf.get_variable(shape=[1, n_classes], initializer = tf.contrib.layers.xavier_initializer(), name='w1_tf')\n",
    "b1_tf = tf.get_variable(shape=[n_classes], initializer = tf.contrib.layers.xavier_initializer(), name='b1_tf')\n",
    "z1_tf = tf.add(tf.matmul(x_data_tf, w1_tf), b1_tf, name='z1_tf')\n",
    "a1_tf = tf.nn.softmax(z1_tf, name='a1_tf')\n",
    "\n",
    "# Loss function\n",
    "loss_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_data_tf, logits=z1_tf), name='loss_tf')\n",
    "\n",
    "# Compute gradients.\n",
    "grad_w1_tf, grad_b1_tf = tf.gradients(xs=[w1_tf, b1_tf], ys=loss_tf)\n",
    "new_w1_tf = w1_tf.assign(w1_tf - learning_rate * grad_w1_tf)\n",
    "new_b1_tf = b1_tf.assign(b1_tf - learning_rate * grad_b1_tf)\n",
    "\n",
    "# Optimization.\n",
    "#optimize_tf = tf.train.AdamOptimizer(learning_rate=0.11).minimize(loss_tf, name='optimize_tf')\n",
    "\n",
    "# Tensor of correct predictions\n",
    "correct_tf = tf.equal(tf.argmax(a1_tf,1), tf.argmax(y_data_tf,1), name='correct_tf')  \n",
    "\n",
    "# Score: accuracy\n",
    "score_tf = tf.reduce_mean(tf.cast(correct_tf, dtype=tf.float32), name='score_tf')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize global variables of graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        \n",
    "        # loss and score\n",
    "        train_loss, train_score = sess.run([loss_tf, score_tf], feed_dict = {x_data_tf:x_train, y_data_tf:y_train})\n",
    "        valid_loss, valid_score = sess.run([loss_tf, score_tf], feed_dict = {x_data_tf:x_train, y_data_tf:y_train})\n",
    "        print('{:.3f} epoch: train/valid loss {:.3f}/{:.3f}, train/valid score {:.3f}/{:.3f}'.format(\n",
    "                        epoch, train_loss, valid_loss, train_score, valid_score))            \n",
    "        #print(sess.run([w1_tf, b1_tf]))\n",
    "        \n",
    "        # Run tf optimizer.\n",
    "        #sess.run(optimize_tf, feed_dict={x_data_tf: x_train, y_data_tf: y_train})\n",
    "\n",
    "        # Run own gradient descent.\n",
    "        sess.run([new_w1_tf, new_b1_tf], feed_dict={x_data_tf: x_train, y_data_tf: y_train})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the graph from scratch and perform MLP classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028 epoch: train/valid loss 0.782/0.734, train/valid score 0.440/0.495\n",
      "1.028 epoch: train/valid loss 0.652/0.612, train/valid score 0.620/0.665\n",
      "2.028 epoch: train/valid loss 0.518/0.521, train/valid score 0.760/0.780\n",
      "3.028 epoch: train/valid loss 0.468/0.457, train/valid score 0.760/0.805\n",
      "4.028 epoch: train/valid loss 0.410/0.416, train/valid score 0.840/0.840\n",
      "5.028 epoch: train/valid loss 0.387/0.392, train/valid score 0.860/0.855\n",
      "6.028 epoch: train/valid loss 0.257/0.378, train/valid score 0.940/0.855\n",
      "7.028 epoch: train/valid loss 0.388/0.370, train/valid score 0.800/0.850\n",
      "8.028 epoch: train/valid loss 0.382/0.366, train/valid score 0.840/0.850\n",
      "9.028 epoch: train/valid loss 0.378/0.365, train/valid score 0.820/0.855\n",
      "10.028 epoch: train/valid loss 0.265/0.364, train/valid score 0.880/0.855\n",
      "11.028 epoch: train/valid loss 0.305/0.364, train/valid score 0.880/0.855\n",
      "12.028 epoch: train/valid loss 0.342/0.365, train/valid score 0.880/0.850\n",
      "13.028 epoch: train/valid loss 0.495/0.365, train/valid score 0.780/0.850\n",
      "14.028 epoch: train/valid loss 0.306/0.366, train/valid score 0.880/0.850\n",
      "15.028 epoch: train/valid loss 0.372/0.367, train/valid score 0.880/0.845\n",
      "16.028 epoch: train/valid loss 0.379/0.367, train/valid score 0.860/0.845\n",
      "17.028 epoch: train/valid loss 0.218/0.368, train/valid score 0.900/0.840\n",
      "18.028 epoch: train/valid loss 0.419/0.368, train/valid score 0.860/0.845\n",
      "19.028 epoch: train/valid loss 0.243/0.369, train/valid score 0.880/0.845\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build the graph from scratch and perform sophisticated MLP classification.\n",
    "\"\"\"\n",
    "\n",
    "learn_rate = 0.001            # learn rate\n",
    "idx_in_epoch = 0              # current index in epoch\n",
    "mb_size = 50                  # mini batch size\n",
    "epoch = 0.                    # current epoch\n",
    "\n",
    "def get_variable(shape, name=None):\n",
    "    \"\"\"Weight or bias initialization.\"\"\"\n",
    "    #return tf.Variable(tf.random_normal(shape=shape, stddev=0.1),name=name)\n",
    "    #return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),name=name)\n",
    "    #initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.get_variable(name=name, shape=shape, initializer=initializer)\n",
    "\n",
    "tf.reset_default_graph()  \n",
    "#graph = tf.Graph()\n",
    "#with graph.as_default():\n",
    "            \n",
    "# Input layer\n",
    "x_data_tf = tf.placeholder(tf.float32, [None, 1], 'x_data_tf')\n",
    "y_data_tf = tf.placeholder(tf.int32, [None, n_classes], 'y_data_tf')\n",
    "\n",
    "# Hidden layer\n",
    "w1_tf = get_variable(shape=[1, n_hid_1], name='w1_tf')\n",
    "b1_tf = get_variable(shape=[n_hid_1], name='b1_tf')\n",
    "z1_tf = tf.add(tf.matmul(x_data_tf, w1_tf), b1_tf, name='z1_tf')\n",
    "a1_tf = tf.nn.relu(z1_tf, name='a1_tf')\n",
    "\n",
    "# Output layer\n",
    "w2_tf = get_variable(shape=[n_hid_1, n_classes], name='w2_tf')\n",
    "b2_tf = get_variable(shape=[n_classes], name='b2_tf')\n",
    "z2_tf = tf.add(tf.matmul(a1_tf, w2_tf), b2_tf, name='z2_tf')\n",
    "\n",
    "# Softmax result in terms of probablities (\"one-hot\" encoding)\n",
    "y_prob_tf = tf.nn.softmax(z2_tf, name='y_prob_tf')\n",
    "\n",
    "# Dense results in terms of classes (dense encoding)\n",
    "y_class_tf = tf.argmax(y_prob_tf, 1, name='y_class_tf')\n",
    "\n",
    "# Loss function\n",
    "loss_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_data_tf, logits=z2_tf), name='loss_tf')\n",
    "#loss_tf = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_data_tf, predictions=y_prob_tf), name='loss_tf')\n",
    "\n",
    "# Optimization.\n",
    "optimize_tf = tf.train.AdamOptimizer(learning_rate=learn_rate).minimize(loss_tf, name='optimize_tf')\n",
    "#optimize_tf = tf.train.RMSPropOptimizer(learning_rate=learn_rate).minimize(loss_tf, name='optimize_tf')\n",
    "\n",
    "# Tensor of correct predictions\n",
    "correct_tf = tf.equal(y_class_tf, tf.argmax(y_data_tf,1), name='correct_tf')  \n",
    "\n",
    "# Score: accuracy\n",
    "score_tf = tf.reduce_mean(tf.cast(correct_tf, dtype=tf.float32), name='score_tf')\n",
    "\n",
    "# add summaries to tf tensors for tensorboard\n",
    "tf.summary.scalar('loss_tf', loss_tf)\n",
    "tf.summary.scalar('score_tf', score_tf)\n",
    "merged_tf = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Saver object for saving the graph and session\n",
    "    saver_tf = tf.train.Saver()\n",
    "    \n",
    "    # Initialize summary writer for tensorboard\n",
    "    timestamp = datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "    train_writer_tf = tf.summary.FileWriter(os.path.join(os.getcwd(),'logs',timestamp,'train'), sess.graph)\n",
    "    valid_writer_tf = tf.summary.FileWriter(os.path.join(os.getcwd(),'logs',timestamp,'valid'), sess.graph)\n",
    "    \n",
    "    # Initialize global variables of graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use 10-fold cross validation.\n",
    "    cv_num = 10\n",
    "    kfold = sklearn.model_selection.KFold(cv_num, shuffle=True, random_state=r_seed)\n",
    "\n",
    "    for i,(train_ids, valid_ids) in enumerate(kfold.split(x_train)):\n",
    "        \n",
    "        # Samples used for training and evaluation\n",
    "        x_trn = x_train[train_ids]\n",
    "        y_trn = y_train[train_ids]\n",
    "        x_vld = x_train[valid_ids]\n",
    "        y_vld = y_train[valid_ids]\n",
    "        \n",
    "        # Reset global parameters. \n",
    "        epoch = 0.\n",
    "        idx_in_epoch = 0\n",
    "        ids = np.arange(len(x_trn))\n",
    "        \n",
    "        # Train the MLP classifier\n",
    "        for n in range(int(20*len(x_trn)/mb_size)):\n",
    "\n",
    "            x_batch, y_batch = get_next_mini_batch()\n",
    "            sess.run(optimize_tf, feed_dict={x_data_tf: x_batch, y_data_tf: y_batch})\n",
    "\n",
    "            # evaluate all summaries attached to tensors and save them\n",
    "            summary = sess.run(merged_tf, feed_dict = {x_data_tf:x_batch, y_data_tf:y_batch})\n",
    "            train_writer_tf.add_summary(summary, n)\n",
    "            summary = sess.run(merged_tf, feed_dict = {x_data_tf:x_vld, y_data_tf:y_vld})\n",
    "            valid_writer_tf.add_summary(summary, n)\n",
    "            \n",
    "            if n%int(1*len(x_trn)/mb_size)==0:\n",
    "                train_loss, train_score = sess.run([loss_tf, score_tf],\n",
    "                                                   feed_dict = {x_data_tf:x_batch, y_data_tf:y_batch})\n",
    "                valid_loss, valid_score = sess.run([loss_tf, score_tf],\n",
    "                                                   feed_dict = {x_data_tf:x_vld, y_data_tf:y_vld})\n",
    "                print('{:.3f} epoch: train/valid loss {:.3f}/{:.3f}, train/valid score {:.3f}/{:.3f}'.format(\n",
    "                    epoch, train_loss, valid_loss, train_score, valid_score))\n",
    "                \n",
    "        break\n",
    "        \n",
    "    # Save graph and session\n",
    "    saver_tf.save(sess, os.path.join(os.getcwd(), 'saves', 'mlp_layer_1'))\n",
    "    \n",
    "    # Close summary writer for tensorboard\n",
    "    train_writer_tf.close()\n",
    "    valid_writer_tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/raoul/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "2018-07-29 13:50:20.108445: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "TensorBoard 1.7.0 at http://Raouls-MBP.fritz.box:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# use Tensorboard for visualization\n",
    "if True:\n",
    "    !tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load an existing graph and test its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saves/mlp_layer_1\n",
      "\n",
      "Predictions of loaded graph:\n",
      "train/test loss 0.991/0.986, train/test score 0.500/0.500\n",
      "\n",
      "Confusion matrix for training samples:\n",
      "Prediction     0     1\n",
      "Truth                 \n",
      "1           1000  1000\n",
      "\n",
      "Confusion matrix for test samples:\n",
      "Prediction     0     1\n",
      "Truth                 \n",
      "1           1000  1000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load an existing graph and test its predictions.\n",
    "\"\"\"\n",
    "\n",
    "# Reset current graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Load existing graph.\n",
    "filepath = os.path.join('.', 'saves', 'mlp_layer_1')\n",
    "saver_tf = tf.train.import_meta_graph(filepath + '.meta')\n",
    "\n",
    "# Load tensors from graph.\n",
    "graph = tf.get_default_graph() # save default graph\n",
    "x_data_tf = graph.get_tensor_by_name(\"x_data_tf:0\")\n",
    "y_data_tf = graph.get_tensor_by_name(\"y_data_tf:0\")\n",
    "loss_tf = graph.get_tensor_by_name(\"loss_tf:0\")\n",
    "score_tf = graph.get_tensor_by_name(\"score_tf:0\")\n",
    "y_class_tf = graph.get_tensor_by_name(\"y_class_tf:0\")\n",
    "\n",
    "# Create new session with loaded graph.\n",
    "sess = tf.Session() # default session\n",
    "saver_tf.restore(sess, filepath) # restore session\n",
    "\n",
    "# Predictions of loaded graph.\n",
    "y_train_pred, train_loss, train_score = sess.run([y_class_tf, loss_tf, score_tf], \n",
    "                                                 feed_dict={x_data_tf:x_train, y_data_tf:y_train})\n",
    "y_test_pred, test_loss, test_score = sess.run([y_class_tf, loss_tf, score_tf], \n",
    "                                              feed_dict={x_data_tf:x_test, y_data_tf:y_test})\n",
    "\n",
    "# Print losses and scores.\n",
    "print('\\nPredictions of loaded graph:\\ntrain/test loss {:.3f}/{:.3f}, train/test score {:.3f}/{:.3f}'.format(\n",
    "    train_loss, test_loss, train_score, test_score))\n",
    "\n",
    "# Confusion matrices.\n",
    "y_truth = pd.Series(y_train_pred, name='Truth')\n",
    "y_pred = pd.Series(one_hot_to_dense(y_train), name='Prediction')\n",
    "df_confusion = pd.crosstab(y_truth, y_pred)\n",
    "print('\\nConfusion matrix for training samples:')\n",
    "print(df_confusion)\n",
    "\n",
    "y_truth = pd.Series(y_test_pred, name='Truth')\n",
    "y_pred = pd.Series(one_hot_to_dense(y_test), name='Prediction')\n",
    "df_confusion = pd.crosstab(y_truth, y_pred)\n",
    "print('\\nConfusion matrix for test samples:')\n",
    "print(df_confusion)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load an existing graph, extend it by one layer and retrain the entire graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saves/mlp_layer_1\n",
      "Initialize:  ['w2_tf_1:0', 'b2_tf_1:0', 'w3_tf:0', 'b3_tf:0', 'w1_tf/RMSProp:0', 'w1_tf/RMSProp_1:0', 'b1_tf/RMSProp:0', 'b1_tf/RMSProp_1:0', 'w2_tf_1/RMSProp:0', 'w2_tf_1/RMSProp_1:0', 'b2_tf_1/RMSProp:0', 'b2_tf_1/RMSProp_1:0', 'w3_tf/RMSProp:0', 'w3_tf/RMSProp_1:0', 'b3_tf/RMSProp:0', 'b3_tf/RMSProp_1:0']\n",
      "0.028 epoch: train/valid loss 0.696/0.699, train/valid score 0.520/0.490\n",
      "1.028 epoch: train/valid loss 0.696/0.697, train/valid score 0.520/0.490\n",
      "2.028 epoch: train/valid loss 0.687/0.688, train/valid score 0.700/0.720\n",
      "3.028 epoch: train/valid loss 0.661/0.662, train/valid score 0.860/0.820\n",
      "4.028 epoch: train/valid loss 0.611/0.601, train/valid score 0.780/0.825\n",
      "5.028 epoch: train/valid loss 0.466/0.522, train/valid score 0.920/0.830\n",
      "6.028 epoch: train/valid loss 0.427/0.449, train/valid score 0.880/0.835\n",
      "7.028 epoch: train/valid loss 0.430/0.401, train/valid score 0.740/0.830\n",
      "8.028 epoch: train/valid loss 0.333/0.374, train/valid score 0.880/0.835\n",
      "9.028 epoch: train/valid loss 0.308/0.365, train/valid score 0.880/0.855\n",
      "10.028 epoch: train/valid loss 0.251/0.364, train/valid score 0.920/0.855\n",
      "11.028 epoch: train/valid loss 0.243/0.365, train/valid score 0.900/0.850\n",
      "12.028 epoch: train/valid loss 0.251/0.366, train/valid score 0.900/0.855\n",
      "13.028 epoch: train/valid loss 0.337/0.367, train/valid score 0.860/0.850\n",
      "14.028 epoch: train/valid loss 0.184/0.368, train/valid score 0.960/0.850\n",
      "15.028 epoch: train/valid loss 0.241/0.369, train/valid score 0.900/0.855\n",
      "16.028 epoch: train/valid loss 0.274/0.369, train/valid score 0.880/0.850\n",
      "17.028 epoch: train/valid loss 0.300/0.370, train/valid score 0.880/0.850\n",
      "18.028 epoch: train/valid loss 0.328/0.371, train/valid score 0.820/0.855\n",
      "19.028 epoch: train/valid loss 0.450/0.371, train/valid score 0.760/0.850\n",
      "\n",
      "Predictions of loaded graph:\n",
      "train/test loss 0.339/0.333, train/test score 0.851/0.862\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load an existing graph and extend the graph by one additional layer.\n",
    "\"\"\"\n",
    "\n",
    "learn_rate = 0.001            # learn rate\n",
    "idx_in_epoch = 0              # current index in epoch\n",
    "mb_size = 50                  # mini batch size\n",
    "epoch = 0.                    # current epoch\n",
    "n_hid2 = 10  \n",
    "\n",
    "def initialize_uninitialized(sess):\n",
    "    \"\"\"Initialize unitialized variables\"\"\"\n",
    "    global_vars          = tf.global_variables()\n",
    "    is_not_initialized   = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    print('Initialize: ',[str(i.name) for i in not_initialized_vars])\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "\n",
    "# Reset current graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Load existing graph.\n",
    "filepath = os.path.join('.', 'saves', 'mlp_layer_1')\n",
    "saver_tf = tf.train.import_meta_graph(filepath + '.meta')\n",
    "\n",
    "# Load tensors from graph.\n",
    "graph = tf.get_default_graph() # save default graph\n",
    "a1_tf = graph.get_tensor_by_name('a1_tf:0')\n",
    "y_data_tf = graph.get_tensor_by_name('y_data_tf:0')\n",
    "x_data_tf = graph.get_tensor_by_name('x_data_tf:0')\n",
    "\n",
    "# Fix a1_tf, i.e. no gradient descent beyond this point.\n",
    "# Otherwise, the whole graph will be retrained\n",
    "#a1_tf = tf.stop_gradient(a1_tf)\n",
    "\n",
    "# Additional second layer.\n",
    "w2_tf = tf.Variable(tf.truncated_normal([a1_tf.shape[1].value,n_hid2], stddev=0.1), name='w2_tf')\n",
    "b2_tf = tf.Variable(tf.truncated_normal([n_hid2], stddev=0.1), name='b2_tf')\n",
    "z2_tf = tf.add(tf.matmul(a1_tf,w2_tf), b2_tf, name='z2_tf')\n",
    "a2_tf = tf.nn.relu(z2_tf, name='a2_tf')\n",
    "\n",
    "# Output layer.\n",
    "w3_tf = tf.Variable(tf.truncated_normal([a2_tf.shape[1].value,n_classes], stddev=0.1), name='w3_tf')\n",
    "b3_tf = tf.Variable(tf.truncated_normal([n_classes], stddev=0.1), name='b3_tf')\n",
    "z3_tf = tf.add(tf.matmul(a2_tf,w3_tf), b3_tf, name='z3_tf')\n",
    "\n",
    "# Softmax result in terms of probablities (\"one-hot\" encoding)\n",
    "y_prob_tf = tf.nn.softmax(z3_tf, name='y_prob_tf')\n",
    "\n",
    "# Dense results in terms of classes (dense encoding)\n",
    "y_class_tf = tf.argmax(y_prob_tf, 1, name='y_class_tf')\n",
    "\n",
    "# Loss function\n",
    "loss_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_data_tf, logits=z3_tf), name='loss_tf')\n",
    "\n",
    "# Optimization.\n",
    "#optimize_tf = tf.train.AdamOptimizer(learning_rate=learn_rate).minimize(loss_tf, name='optimize_tf')\n",
    "optimize_tf = tf.train.RMSPropOptimizer(learning_rate=learn_rate).minimize(loss_tf, name='optimize_tf')\n",
    "\n",
    "# Tensor of correct predictions\n",
    "correct_tf = tf.equal(y_class_tf, tf.argmax(y_data_tf,1), name='correct_tf')  \n",
    "\n",
    "# Score: accuracy\n",
    "score_tf = tf.reduce_mean(tf.cast(correct_tf, dtype=tf.float32), name='score_tf')\n",
    "\n",
    "# Create new session with loaded graph.\n",
    "sess = tf.Session() # default session\n",
    "saver_tf.restore(sess, filepath) # restore session\n",
    "\n",
    "# Initialize only uninitialized variables\n",
    "initialize_uninitialized(sess)\n",
    "\n",
    "# Use 10-fold cross validation.\n",
    "cv_num = 10\n",
    "kfold = sklearn.model_selection.KFold(cv_num, shuffle=True, random_state=r_seed)\n",
    "\n",
    "for i,(train_ids, valid_ids) in enumerate(kfold.split(x_train)):\n",
    "\n",
    "    # Samples used for training and evaluation\n",
    "    x_trn = x_train[train_ids]\n",
    "    y_trn = y_train[train_ids]\n",
    "    x_vld = x_train[valid_ids]\n",
    "    y_vld = y_train[valid_ids]\n",
    "\n",
    "    # Reset global parameters. \n",
    "    epoch = 0.\n",
    "    idx_in_epoch = 0\n",
    "    ids = np.arange(len(x_trn))\n",
    "\n",
    "    # Train the MLP classifier\n",
    "    for n in range(int(20*len(x_trn)/mb_size)):\n",
    "\n",
    "        x_batch, y_batch = get_next_mini_batch()\n",
    "        sess.run(optimize_tf, feed_dict={x_data_tf: x_batch, y_data_tf: y_batch})\n",
    "\n",
    "        if n%int(1*len(x_trn)/mb_size)==0:\n",
    "            train_loss, train_score = sess.run([loss_tf, score_tf],\n",
    "                                               feed_dict = {x_data_tf:x_batch, y_data_tf:y_batch})\n",
    "            valid_loss, valid_score = sess.run([loss_tf, score_tf],\n",
    "                                               feed_dict = {x_data_tf:x_vld, y_data_tf:y_vld})\n",
    "            print('{:.3f} epoch: train/valid loss {:.3f}/{:.3f}, train/valid score {:.3f}/{:.3f}'.format(\n",
    "                epoch, train_loss, valid_loss, train_score, valid_score))\n",
    "    break\n",
    "\n",
    "# Predictions of loaded graph.\n",
    "y_train_pred, train_loss, train_score = sess.run([y_class_tf, loss_tf, score_tf], \n",
    "                                                 feed_dict={x_data_tf:x_train, y_data_tf:y_train})\n",
    "y_test_pred, test_loss, test_score = sess.run([y_class_tf, loss_tf, score_tf], \n",
    "                                              feed_dict={x_data_tf:x_test, y_data_tf:y_test})\n",
    "# Print losses and scores.\n",
    "print('\\nPredictions of loaded graph:\\ntrain/test loss {:.3f}/{:.3f}, train/test score {:.3f}/{:.3f}'.format(\n",
    "    train_loss, test_loss, train_score, test_score))\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Try out other Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg: train/valid accuracy = 0.851/0.845\n",
      "random_forest: train/valid accuracy = 0.979/0.800\n",
      "extra_trees: train/valid accuracy = 1.000/0.780\n",
      "gradient_boost: train/valid accuracy = 0.877/0.840\n",
      "decision_tree: train/valid accuracy = 1.000/0.795\n",
      "gaussianNB: train/valid accuracy = 0.851/0.835\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Try different other classifiers.\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "\n",
    "## First try out some basic sklearn models\n",
    "logreg = sklearn.linear_model.LogisticRegression(verbose=0, solver='lbfgs', multi_class='multinomial')\n",
    "decision_tree = sklearn.tree.DecisionTreeClassifier()\n",
    "extra_trees = sklearn.ensemble.ExtraTreesClassifier(verbose=0)\n",
    "gradient_boost = sklearn.ensemble.GradientBoostingClassifier(verbose=0)\n",
    "random_forest = sklearn.ensemble.RandomForestClassifier(verbose=0)\n",
    "gaussianNB = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "# store models in dictionary\n",
    "base_models = {'logreg': logreg, 'extra_trees': extra_trees,\n",
    "               'gradient_boost': gradient_boost, 'random_forest': random_forest, \n",
    "               'decision_tree': decision_tree, 'gaussianNB': gaussianNB}\n",
    "\n",
    "# choose models for out-of-folds predictions\n",
    "take_models = ['logreg','random_forest','extra_trees', 'gradient_boost', 'decision_tree', 'gaussianNB']\n",
    "\n",
    "# Cross validations.\n",
    "cv_num = 10  \n",
    "kfold = sklearn.model_selection.KFold(cv_num, shuffle=True, random_state=r_seed)\n",
    "\n",
    "for i,(train_ids, valid_ids) in enumerate(kfold.split(x_train)):\n",
    "\n",
    "    # Samples used for training and evaluation.\n",
    "    x_trn = x_train[train_ids]\n",
    "    y_trn = y_train[train_ids]\n",
    "    x_vld = x_train[valid_ids]\n",
    "    y_vld = y_train[valid_ids]\n",
    "    \n",
    "    for mn in take_models:\n",
    "\n",
    "        # Create cloned model from base models.\n",
    "        model = sklearn.base.clone(base_models[mn])\n",
    "        model.fit(x_trn, one_hot_to_dense(y_trn))\n",
    "\n",
    "        # Predictions.\n",
    "        y_train_pred = model.predict(x_trn)\n",
    "        y_valid_pred = model.predict(x_vld)\n",
    "        \n",
    "        print('{}: train/valid accuracy = {:.3f}/{:.3f}'.format(mn, np.mean(y_train_pred == np.argmax(y_trn,1)),\n",
    "                                                  np.mean(y_valid_pred == np.argmax(y_vld,1))))\n",
    "    \n",
    "    break;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
